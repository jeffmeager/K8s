pipeline {
    agent any

    environment {
        PATH                = "/opt/homebrew/bin:/usr/local/bin:${env.PATH}"
        AWS_DEFAULT_REGION  = 'us-east-1'
        EKS_CLUSTER_NAME    = 'challenge-eks-cluster'
    }

    stages {
        stage('Teardown Pipeline') {
            steps {
                withCredentials([
                    string(credentialsId: 'aws-access-key-id', variable: 'AWS_ACCESS_KEY_ID'),
                    string(credentialsId: 'aws-secret-access-key', variable: 'AWS_SECRET_ACCESS_KEY'),
                    string(credentialsId: 'cloudflare_api_token', variable: 'TF_VAR_cloudflare_api_token')
                ]) {
                    script {
                        // Sub-stage 1: Conditional kubectl cleanup if cluster exists
                        sh '''
                            echo "üîç Checking if EKS cluster '${EKS_CLUSTER_NAME}' exists..."
                            if aws eks describe-cluster --name ${EKS_CLUSTER_NAME} --region ${AWS_DEFAULT_REGION} >/dev/null 2>&1; then
                                echo "‚úÖ Cluster found. Updating kubeconfig..."
                                aws eks update-kubeconfig \
                                    --name ${EKS_CLUSTER_NAME} \
                                    --region ${AWS_DEFAULT_REGION}

                                echo "‚úÖ Running Kubernetes deletes..."
                                kubectl delete -f kubernetes/deployments/webapp-deployment.yaml || true
                                sleep 10
                                kubectl delete -f kubernetes/services/webapp-service.yaml || true
                                kubectl delete -f kubernetes/deployments/cluster-admin-rolebinding.yaml || true
                                sleep 10

                                echo "üßπ Uninstalling Helm charts..."
                                helm uninstall csi-secrets-store -n kube-system || true
                                helm uninstall secrets-store-csi-driver-provider-aws -n kube-system || true
                            else
                                echo "‚ö†Ô∏è Cluster '${EKS_CLUSTER_NAME}' not found. Skipping kubeconfig and Kubernetes deletes."
                            fi
                        '''

                        // Sub-stage 2: Always run ELB cleanup (even if cluster is gone. Found webapp-service.yaml doesn't always complete)
                        sh '''
                            echo "üîç Looking for Classic ELB tagged by Kubernetes for webapp-service..."

                            # Get all Classic ELB names
                            for ELB_NAME in $(aws elb describe-load-balancers --region us-east-1 --query "LoadBalancerDescriptions[].LoadBalancerName" --output text); do
                                echo "Checking ELB: $ELB_NAME"
                                TAGS=$(aws elb describe-tags --region us-east-1 --load-balancer-names "$ELB_NAME" --query "TagDescriptions[].Tags" --output json)

                                # Look for tag "kubernetes.io/service-name" == "default/webapp-service"
                                if echo "$TAGS" | grep -q '"Key": "kubernetes.io/service-name"'; then
                                    if echo "$TAGS" | grep -q '"Value": "default/webapp-service"'; then
                                        echo "‚úÖ Found matching ELB: $ELB_NAME ‚Äî deleting it..."
                                        aws elb delete-load-balancer --region us-east-1 --load-balancer-name "$ELB_NAME"

                                        echo "‚è≥ Waiting for ELB $ELB_NAME to be deleted..."
                                        while aws elb describe-load-balancers --region us-east-1 --load-balancer-names "$ELB_NAME" >/dev/null 2>&1; do
                                            echo "Still waiting for ELB $ELB_NAME to be deleted..."
                                            sleep 10
                                        done
                                        echo "üéâ ELB $ELB_NAME deleted."
                                    fi
                                fi
                            done
                            echo "‚úÖ ELB cleanup complete."
                        
                            echo "üîç Looking up VPC ID for SG cleanup..."

                            # Lookup VPC ID by tag Name=Challenge
                            VPC_ID=$(aws ec2 describe-vpcs --region us-east-1 \
                                --filters Name=tag:Name,Values='Challenge' \
                                --query "Vpcs[0].VpcId" --output text)

                            if [ -z "$VPC_ID" ] || [ "$VPC_ID" = "None" ]; then
                                echo "‚ùå Could not determine VPC ID ‚Äî skipping SG cleanup."
                            else
                                echo "‚úÖ VPC ID is $VPC_ID"
                                echo "üîç Looking for orphaned Kubernetes ELB security groups in VPC $VPC_ID..."

                                SG_IDS=$(aws ec2 describe-security-groups --region us-east-1 \
                                    --filters Name=vpc-id,Values=$VPC_ID \
                                            Name=group-name,Values="*k8s-elb-*" \
                                    --query "SecurityGroups[?GroupName!='default'].GroupId" --output text)

                                if [ -z "$SG_IDS" ]; then
                                    echo "‚úÖ No orphaned Kubernetes ELB security groups found."
                                else
                                    for SG_ID in $SG_IDS; do
                                        echo "üóëÔ∏è Deleting orphaned Kubernetes ELB security group: $SG_ID"
                                        aws ec2 delete-security-group --region us-east-1 --group-id "$SG_ID" || echo "‚ö†Ô∏è Could not delete SG $SG_ID ‚Äî may still be in use."
                                    done
                                    echo "‚úÖ Security group cleanup complete."
                                fi
                            fi
                        '''
                        // Sub-stage 3: Empty S3 buckets
                            sh(script: 'aws s3 rm s3://challenge-docker-backups --recursive', returnStatus: true)
                            sh(script: 'aws s3 rm s3://challenge-cloudtrail-logs --recursive', returnStatus: true)

                        // Sub-stage 4: Terraform Init
                        dir('terraform') {
                            sh '''
                                terraform init -backend-config="bucket=jeffmeager-challenge-terraform-state-bucket" \
                                                -backend-config="key=challenge/terraform.tfstate" \
                                                -backend-config="region=ap-southeast-2"
                            '''
                        }

                        // Sub-stage 5: Terraform Destroy
                        withCredentials([
                            string(credentialsId: 'mongodb-username', variable: 'TF_VAR_mongodb_username'),
                            string(credentialsId: 'mongodb-password', variable: 'TF_VAR_mongodb_password')
                        ]) {
                            dir('terraform') {
                                sh '''
                                    terraform destroy -auto-approve \
                                        -var region=${AWS_DEFAULT_REGION}
                                '''
                            }
                        }
                        // Sub-stage 6: Cleanup AWS Secrets Manager
                        sh '''
                            aws secretsmanager delete-secret \
                                --secret-id webapp-secrets \
                                --force-delete-without-recovery || true
                        '''
                    }
                }
            }
        }
    }

    post {
        success {
            echo 'üßπ Infrastructure and workloads successfully cleaned up'
        }
        failure {
            echo '‚ö†Ô∏è Something went wrong while tearing down'
        }
    }
}
